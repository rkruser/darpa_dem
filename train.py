"""
This script runs a syllabus for classification data tasks, using a pre-generated HDF5 datafile. This HDF5 file can be
generated by running

    python examples/classification_tasks/datagen/l2a_gen.py

(See README or actual code for more information on use.)

The L2DATA environment variable should be set before running this script (see classification_tasks README), which
specifies the top-level folder under which to look for the pre-generated data. If not set, the default is 'l2data' under
the root directory, e.g. ~/l2data on Linux/Mac.

Note that this code is purely illustrative in regards to acquiring data from the dataset to train a model, no actual
model training occurs here.
"""

import numpy as np
import torch
from torch.utils.data import DataLoader

import learnkit
from learnkit.utils import module_relative_file

from models import OurRewardPredictor, OurOptimizer
from dataloader import L2M_Pytorch_Dataset


def train_model(path_to_syllabus):
    # Instantiate Model, optimizer, training parameters, etc.
    model = OurRewardPredictor(color_adversary=False)
    optim = OurOptimizer(model)
    dset = DataLoader(L2M_Pytorch_Dataset('gen_syllabus.json'), batch_size=32, shuffle=True, num_workers=4)

    cl = learnkit.Classroom()
    with cl.load(path_to_syllabus) as syllabus:
        syllabus.reset()

        # get all input and output spaces for all tasks in the syllabus
        task_spaces = syllabus.task_spaces
        print(task_spaces)
        model.set_task_spaces(task_spaces)

        for dataset in syllabus.datasets():
            done = False

            # The underlying dataset needs to be reset so that the parameters are passed to it.
            dataset.reset()

            # access information about the dataset.

            # e.g. name of current task
            task_name = dataset.info.name
            print(task_name)

            # e.g. {'4_frames': Box(3, 4, 128, 128)}
            model.set_input_space(dataset.get_input_spaces())

            # e.g. {'game': Discrete(6)}
            model.set_output_space(dataset.get_output_spaces())

            print(dataset.get_input_spaces())
            print(dataset.get_output_spaces())

            dataset.log.debug(dataset.info.name)
            while not done:
                # batch only includes input information into the network and a batch id to retrieve batch labels later.
                # Inference should done on this data and the result passed to the get_labels() method below to
                # receive labels for the batch.
                batch, done, info = dataset.next_batch()

                if not batch:
                    # all the data has been returned. We only get here if the batch size evenly divides the
                    # number of examples in the dataset
                    break

                # batch ids are used to collect label data. This allows a user to call next_batch() multiple
                # times before get_labels(), or enable training in parallel.
                batch_id = batch['id']

                # x is a dict: e.g. {'4_frames': np.array of shape (batch_size, 3, 4, 128, 128) }
                #   in this example, the examples consist of 4 frames of 128x128x3 images
                x = batch['inputs']

                # perform inference here
                #   y_hat should be a python dictionary with one numpy array of estimates for each key in the
                #   output_space, e.g. {'game': np.array of shape (batch_size,)}
                if '10_frames' not in x:
                    break
                y_hat = model.forward(x)

                # get_labels() collects y_hat for performance analysis purposes before the labels are provided. This
                # forces the user to provide unbiased estimates to the framework before getting ground truth.
                y = dataset.get_labels(batch_id, y_hat)

                # the actual key, pairs of y and y_hat will vary depending on the actual dataset

                # y may also be None if no outputs are presented; i.e. during testing/evaluation
                if y:
                    # perform training
                    loss = optim.calculate_loss(y, y_hat)
                    model.update(loss)
                    dataset.log.debug("    updates enabled")
                else:
                    # labels will be None during inference only datasets, i.e. test data
                    dataset.log.debug(" ***updates *DIS*abled")


if __name__ == "__main__":
    import argparse
    syllabi = ['predict_task_syllabus', 'train_predict_total_reward_syllabus', 'predict_frame_n_syllabus']

    parser = argparse.ArgumentParser(description="Sample classification task training script.")
    parser.add_argument('--syllabus', choices=syllabi, default='train_predict_total_reward_syllabus')
    args = parser.parse_args()

    load_syllabus = args.syllabus
    train_model(module_relative_file(__file__, load_syllabus))
