% IJCAI / L2M research summary

% https://ijcai20.org/call-for-papers.html

% These are the instructions for authors for IJCAI-20.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}

% Fix some error with the bibtex
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym} 

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
%\newtheorem{example}{Example}
%\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning to Predict Based on Causality Using an Adversarial Objective}

\author{
Ryen Krusinga$^1$
\and
David Jacobs$^2$
\affiliations
$^1$University of Maryland\\
$^2$University of Maryland
\emails
\{krusinga, djacobs\}@umiacs.umd.com
}
%\fi

\begin{document}

\maketitle

\begin{abstract}
In a given learning environment, some variables are causal, and some are merely correlated with the prediction targets. Standard machine learning models make no distinction between the two, making the models less robust to domain shift, in which causal factors remain invariant but correlations change. Given background knowledge about the causal structure of the environment, we demonstrate a simple adversarial method to train a predictor that ignores non-causal information. %We show results on two artificial environments.
\end{abstract}


\section{Introduction}
The importance of embedding causal reasoning into machine learning has become increasingly noticed in recent years. Most existing machine learning algorithms establish correlation only; this is usually enough for prediction, but it is not enough for other kinds of reasoning, such as counterfactual reasoning \cite{pearl2018theoretical} \cite{Pearl2009}. Furthermore, causal reasoning is, in principle, 
%both more \emph{efficient} and 
more \emph{generalizable} than reasoning based on correlation only: causal rules presumably hold between different domain-shifted datasets of the same kind even when correlations change \cite{Bottou2019presentation}. 

We conduct a preliminary investigation into training a model using background knowledge of causality in a simple game environment - in this case, Pong. Suppose that two factors can vary - the size of the player's Pong paddle relative to the opponent's, and the background color. Since the transition model in Pong is fully known, we know \emph{a priori} that background color will not affect the gameplay, but paddle size will. (If we did not know this \emph{a priori}, we could figure it out by performing simple experiments in which we intervene randomly on these attributes, then observe the outcomes). Suppose also that we have a biased dataset in which both color and paddle size correlate with outcome. Can we train a model on this biased dataset to ignore the non-causal variable and pay attention only to the causal one?

Many real-world problems come with similar kinds of background knowledge that could potentially be exploited in this way; in the causal literature, this knowledge is explicitly represented by structural causal models \cite{Pearl2009} \cite{pearl2009causalitybook}. For example, we know that lighting conditions do not affect the laws of motion, yet datasets are likely to be biased towards the daytime, potentially introducing spurious correlations between daylight and outcomes. %Find better example?

We propose that this problem could be solved with a simple adversarial objective. We train a network to predict the likelihood of a certain game outcome given a single frame (so that the probability of winning is predicted based only on fixed game attributes); in addition, we add a secondary predictor late in the network that predicts the value of the non-causal variable (in this case, background color). The objective function of the secondary predictor is to minimize the error in its prediction of background color. The objective function of the outcome predictor is to minimize error in its win-probability predictions while maximizing error in the secondary color predictor. This forces the primary network to make its prediction based only on factors unrelated to color, hopefully removing the spurious correlation between color and outcome. We are motivated by the literature on generative adversarial networks, transfer learning, and data anonymization 
\cite{feutry2018learning} \cite{hukkelaas2019deepprivacy} \cite{li2009modeling} %Anonymization
\cite{tan2018survey} \cite{ganin2016domain} % Transfer learning
\cite{goodfellow2014generative} %GANs
.


\section{Background}
Causal inference is the study of counterfactual distributions. Counterfactuals are questions of the form ``What would happen if X?", or ``What would have happened had X been different?" \cite{pearl2018theoretical}. Of central importance is the idea of an ``intervention": a surgical change, whether actually performed or imagined, in the deeper transition model (possibly with hidden states) that generates the observed data, such that all but a small portion of the transition function remains the same. For a concrete example: observation is noticing that wet grass correlates with rain; intervention is wetting the grass with a hose to see if rain starts falling, thus answering the counterfactual question ``Would wetting the grass bring the rain?" (This intervention is surgical in the sense that wetting the grass does not have arbitrarily widespread effects, such as changing the physics of the atmosphere. In general, it is never \emph{certain} whether or not an intervention was truly surgical or local in this sense, but steps like randomized controls can get close).
	
In general, it is not possible to infer causal relations without the ability to interact with the environment and perform experiments. This is, in short, because probability distributions over counterfactual data are underdetermined by observed data - multiple transition models could explain the observed data - and moreover, interventions could potentially have different effects depending on how they are done (e.g., running a study in a lab versus in the field where there may be different background dynamics). However, it is possible, under certain conditions, to infer causal effects from purely observational data. These conditions include either sufficiently detailed background knowledge about the domain from which the data was sampled \cite{Pearl2009} \cite{Rosenbaum1983}, or restricted formalizations such as Granger causality \cite{granger1969investigating} \cite{granger1980testing}. Under such conditions, it is possible to use techniques such as propensity score matching \cite{Rosenbaum1983}, inverse probability weighting \cite{seaman2013review} or directly conditioning on confounding to ascertain causal effects.


% Pasted from workshop_submission to NeurIps
\textbf{Connection to reinforcement learning.} The major problem with causality in machine learning is the fact that data points are high-dimensional, and thus the space of all possible causal graphs is intractable; method of simplifying the problem are needed. A promising direction for high dimensional causal inference is reinforcement learning \cite{Sutton2018}, which is strongly related to causal modeling in that it explicitly represents actions performed in the environment. Actions correspond directly to interventions in some causal model. For example, \cite{Oh2015} uses a reinforcement learning agent to play Atari games. The agent can interact with the environment by generating controller inputs, which directly cause changes in the environment in response; thus the agent learns not just how typical Atari gameplay looks, but also the effects of interventions on the observed video sequences. There is not much overlap in the literature of reinforcement learning and causal inference, though some papers have made promising connections. For example, \cite{Dasgupta2019} uses the method of meta-reinforcement learning to train an agent to predict the effects of interventions on randomly generated causal graphs. Other papers, such as \cite{Lange2012}, contribute methods of embedding high dimensional data into low-dimensional spaces on which action policies can be computed. Such low dimensional representations may be useful for problems of causal inference. A method from outside the reinforcement learning paradigm which achieves a similar kind of embedding is \emph{slow feature analysis} \cite{kompella2011incremental}, which takes high dimensional data and identifies latent causal factors on the assumption that they change more slowly than the observed output stream.

\textbf{Prediction and causality.} There are strong connections between predictive models and causal inference. The connection results from the fact that every predictive model can be regarded as a sort of causal model whose points of intervention are the internal state variables of the predictor function. For example, consider a model that takes $m$ past video frames inside an image space $I$ and predicts $n$ future video frames via the function $F: I^m \rightarrow I^n$. Suppose that $F$ factors as the composition of $G: I^m \rightarrow Z$ and $H: Z \rightarrow I^n$, where $Z$ is a latent space. The function $H$ automatically defines the effects of all possible interventions on $Z$. If $Z$ corresponds to a disentangled, interpretable set of high-level image properties, such as whether or not a car in a video is driving smoothly or erratically, then we can use $H$ to predict the likely effects of interventions without actually having performed any interventions during training. Examples of models that learn disentangled representations include \cite{chen2016infogan} for static images and \cite{denton2017unsupervised} for videos. There are also many models that attempt to directly predict future video frames, such as \cite{Babaeizadeh2017}, \cite{Lotter2016}, \cite{Finn2016}, \cite{Walker2014}. Such models might be combined with causal methods to infer underlying causal structure in video.







% Write this in parallel with coding / cleaning up code
\section{Models and Experiments}


In our experiments, we ask, ``can a model learn to ignore a spurious correlation between a causally irrelevant variable and an outcome?" We use the Darpa L2M Test and Evaluation Framework to generate two datasets, each consisting of the frames from 50 games of Pong between two computer agents. We consistently root for one of the agents, and each frame is labeled with a 0 or a 1 depending on whether our agent wins that game. (Henceforth, we shall simply refer to ``the agent" or ``the opponent"). Table \ref{tab:dataset_proportions} summarizes the two datasets (and their truncated versions, as we tested in practice). The training set varies the paddle size of the agent relative to the opponent, which has a causal effect on the probability of victory. The background color is also varied between red and blue. However, we introduce a spurious correlation between the background color and the outcome, such that games with a red background also have larger agent paddles, and so have higher win percentages. Games with a blue background have smaller paddles, and lower win percentages. In the second ``interventional" or ``domain-shifted" dataset, the spurious correlation is reversed, and now blue games have larger paddles.

\textbf{Models.} Our model consists of a convolutional neural network that maps a $128\times 128 \times 3$ game image to a predicted probability of win or loss for the agent. This model only considers one frame at a time, to assess the causal influence of factors that remain constant throughout the games. There is a separate ``color adversary" model that takes one of the latent feature embeddings later in the conv net and maps it to a prediction of the color of the output. During regular training, the conv net learns to predict win or loss only. During adversarial training, the color adversary predicts the background color from the embedding and attempts to maximize its accuracy, while the conv net is trained to minimize the accuracy of the color adversary while maximizing the accuracy of the win/loss prediction.

\textbf{Experiments and Results.} We train the conv net on the first dataset and test on the second, both with and without the color adversary. We later repeated this training process with the truncated versions of both datasets (as seen in Table \ref{tab:dataset_proportions}) in which the latter 50\% of each game is excluded from the data, so that the networks make their predictions only based on static information in the images, and that our results are not confounded by accuracy gained from the last few frames of each game in which the win or loss is apparent from the position of the ball. Each model is trained for 50 epochs with the Adam optimizer, batch size 64. %Check these numbers

Table \ref{tab:model_performance} shows each model's predictions in each quadrant where data existed. These predictions can be compared to the bolded numbers in Table \ref{tab:dataset_proportions} for the corresponding dataset. Without the adversary, the model is able to predict the quadrant-specific win probabilities quite accurately, on both regular and truncated datasets. However, it is less accurate on the regular and truncated interventional datasets, as expected, although its predictions are directionally correct from the original dataset. This is consistent with the idea that the model learns some predictive information from paddle size, and some from color.

The second half of table \ref{tab:model_performance} shows the effect of the adversarial training.



%%%%% Training category proportions %%%%%%%
% Paddle size \textbackslash Background Color
\begin{table*} % Star does renders the table across two columns.
\centering
\begin{tabular}{r | c | c | c | c | c}
 Dataset & Frames & Red-Large & Red-Small & Blue-Large & Blue-Small \\
 \hline
 %D1 & 2458 & 0.422 / 0.553 & 0.090 / 0.000 & 0.095 / 0.282 & 0.393 / 0.074 \\
 %D1 intervention & 2557 & 0.426 / 0.605 & 0.385 / 0.000 & 0.084 / 0.594 & 0.105 / 0.000 \\
 D1 & 2313 & 0.5275 / \textbf{0.5328} & 0.000 / 0.000 & 0.000 / 0.000 & 0.4725 / \textbf{0.1775} \\
 D1 Intervention & 2205 & 0.000 / 0.000 & 0.3379 / \textbf{0.1624} & 0.6621 / \textbf{0.4521} & 0.000 / 0.000 \\
 Truncated D1 & 1156 & 0.5275 / \textbf{0.5328} & 0.000 / 0.000 & 0.000 / 0.000 & 0.4725 / \textbf{0.1775} \\ 
 Truncated D1 Intervention & 1102 & 0.000 / 0.000 & 0.3379 / \textbf{0.1624} & 0.6621 / \textbf{0.4521} & 0.000 / 0.000 \\
\end{tabular}
\caption{Dataset Info. Proportion of total data points (Pong frames) in category / Proportion of points in category belonging to winning games.}
\label{tab:dataset_proportions}
\end{table*}
%%%%%%%%%%%%

\begin{table*} % Star does renders the table across two columns.
\centering
\begin{tabular}{r | c | c | c | c | c}
 Model type & Dataset & Red-Large & Red-Small & Blue-Large & Blue-Small \\
 \hline
% D1 & 
% D1 intervention & 
No adversary & D1 & 0.5263 & - & - & 0.1871 \\
No adversary  & D1 Intervention & - & 0.2401 & 0.3243 & - \\
No adversary  & Truncated D1 & 0.5293 & - & - & 0.1890 \\
No adversary  & Truncated D1 Intervention & - & 0.3221 & 0.3007 & - \\
\hline
Adversary & D1 & 0.3892 & - & - & 0.3510 \\
Adversary  & D1 Intervention & - & 0.4590 & 0.2926 & - \\
Adversary  & Truncated D1 & 0.3946 & - & - & 0.3635 \\
Adversary  & Truncated D1 Intervention & - & 0.3159 & 0.3807 & -
\end{tabular}
\caption{Predicted winning probabilities learned by each model on each dataset, in each category. Dashes occur where there are zero data points in the given category. Compare with the bolded numbers in Table \ref{tab:dataset_proportions}.}
\label{tab:model_performance}
\end{table*}
%%%%%%%%%%%%



%\section{Results}










\section{Conclusion and Future Work}
We believe that causality is a fruitful area of research. In the future, we intend to explore more dynamic models that account for transitions between frames, as well as models capable of automatically learning the causally relevant variables rather than relying on explicitly programmed background information.








% Acknowledge Darpa L2M
\section{Acknowledgement}

We would like to thank the Lifelong Learning Machines program from DARPA/MTO for their support of this project.











%\nocite{*}
\bibliographystyle{named}
\bibliography{causality_l2m_bib}

\end{document}




