% IJCAI / L2M research summary

% https://ijcai20.org/call-for-papers.html

% These are the instructions for authors for IJCAI-20.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}

% Fix some error with the bibtex
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym} 

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
%\newtheorem{example}{Example}
%\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning to Predict Based on Causality Using an Adversarial Objective}

% Single author syntax
%\author{
%    Christian Bessiere
%    \affiliations
%    CNRS, University of Montpellier, France
%    \emails
%    pcchair@ijcai20.org
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai20-multiauthor.tex file for detailed instructions
%\iffalse
\author{
Ryen Krusinga$^1$
\and
David Jacobs$^2$
\affiliations
$^1$University of Maryland\\
$^2$University of Maryland
\emails
\{krusinga, djacobs\}@umiacs.umd.com
}
%\fi

\begin{document}

\maketitle

\begin{abstract}
In a given learning environment, some variables are causal, and some are merely correlated with the prediction targets. Standard machine learning models make no distinction between the two, making the models less robust to domain shift, in which causal factors remain invariant but correlations change. Given background knowledge about the causal structure of the environment, we demonstrate a simple adversarial method to train a predictor that ignores non-causal information. %We show results on two artificial environments.
\end{abstract}

%This is a test citation \cite{Oh2015}.

\section{Introduction}
The importance of embedding causal reasoning into machine learning has become increasingly noticed in recent years. Most existing machine learning algorithms establish correlation only; this is usually enough for prediction, but it is not enough for other kinds of reasoning, such as counterfactual reasoning \cite{pearl2018theoretical} \cite{Pearl2009}. Furthermore, causal reasoning is, in principle, both more \emph{efficient} and more \emph{generalizable} than reasoning based on correlation only: causal rules presumably hold between different domain-shifted datasets of the same kind even when correlations change \cite{Bottou2019presentation}. 

We conduct a preliminary investigation into training a model using background knowledge of causality in a simple game environment - in this case, Pong. Suppose that two factors can vary - the size of the player's Pong paddle relative to the opponent's, and the background color. Since the transition model in Pong is fully known, we know \emph{a priori} that background color will not affect the gameplay, but paddle size will. (If we did not know this \emph{a priori}, we could figure it out by performing simple experiments in which we intervene randomly on these attributes, then observe the outcomes). Suppose also that we have a biased dataset in which both color and paddle size correlate with outcome. Can we train a model on this biased dataset to ignore the non-causal variable and pay attention only to the causal one?

Many real-world problems come with similar kinds of background knowledge that could potentially be exploited in this way; in the causal literature, this knowledge is explicitly represented by structural causal models \cite{Pearl2009} \cite{pearl2009causalitybook}. For example, we know that lighting conditions do not affect the laws of motion, yet datasets are likely to be biased towards the daytime, potentially introducing spurious correlations [find a better example].

We propose that this problem could be solved with a simple adversarial objective. We train a network to predict the likelihood of a certain game outcome given a single frame (so that the probability of winning is predicted based only on fixed game attributes); in addition, we add a secondary predictor late in the network that predicts the value of the non-causal variable (in this case, background color). The objective function of the secondary predictor is to minimize the error in its prediction of background color. The objective function of the outcome predictor is to minimize error in its win-probability predictions while maximizing error in the secondary color predictor. This forces the primary network to make its prediction based only on factors unrelated to color, hopefully removing the spurious correlation between color and outcome. [Cite GANs and data-privacy learning here?]


\section{Background}
Causal inference is the study of counterfactual distributions. Counterfactuals are questions of the form ``What would happen if X?", or ``What would have happened had X been different?" \cite{pearl2018theoretical}. Of central importance is the idea of an ``intervention": a surgical change, whether actually performed or imagined, in the deeper transition model (possibly with hidden states) that generates the observed data, such that all but a small portion of the transition function remains the same. For a concrete example: observation is noticing that wet grass correlates with rain; intervention is wetting the grass with a hose to see if rain starts falling, thus answering the counterfactual question ``Would wetting the grass bring the rain?" (This intervention is surgical in the sense that wetting the grass does not have arbitrarily widespread effects, such as changing the physics of the atmosphere. In general, it is never \emph{certain} whether or not an intervention was truly surgical or local in this sense, but steps like randomized controls can get close).
	
In general, it is not possible to infer causal relations without the ability to interact with the environment and perform experiments. This is, in short, because probability distributions over counterfactual data are underdetermined by observed data - multiple transition models could explain the observed data - and moreover, interventions could potentially have different effects depending on how they are done (e.g., running a study in a lab versus in the field where there may be different background dynamics). However, it is possible, under certain conditions, to infer causal effects from purely observational data. These conditions include either sufficiently detailed background knowledge about the domain from which the data was sampled \cite{Pearl2009} \cite{Rosenbaum1983}, or restricted formalizations such as Granger causality \cite{granger1969investigating} \cite{granger1980testing}. Under such conditions, it is possible to use techniques such as propensity score matching \cite{Rosenbaum1983}, inverse probability weighting [cite this] or directly conditioning on confounding to ascertain causal effects.

\textbf{Relation to transfer learning}



\textbf{Relation to Reinforcement learning}




\textbf{Relation to predictive models}



\textbf{Background of adversarial scrubbing or something}



\section{Models and Experiments}

%Using the TEF framework, we simulate Pong games, varying the background color (Red or Blue) and the paddle size of the agent relative to the opponent (large or small). Table \ref{tab:dataset_proportions} shows four datasets, consisting of 50 games each, whose frames are labeled with a 1 if the agent wins the game, and a 0 otherwise. Each column shows, for the frames from a particular set of games simulated with the fixed column parameters, the proportion of the total dataset constituted by those frames (left), and the proportion of those frames that are part of a winning game (right, bold). There are four datasets: the original D1, the ``Interventional" data corresponding to a domain shift in data proportions, and truncated versions of each of these. In the truncated versions, the latter half of each game is thrown out, so that the models are forced to distinguish the winner and loser more by looking at the fixed paddle size and color rather than particular ball position, as we are only interested in the causal effect of these factors.

%There are two models, each consisting of the same basic convolutional neural network, but one with an extra adversarial objective that forces the model to not consider color information when making its predictions. Table \ref{tab:model_performance} shows each type of model's learned category-specific winning probability predictions (averaged over data points in that category) after 50 epochs. Ideally, these numbers should match up with the bolded numbers in Table \ref{tab:dataset_proportions}.


%%%%% Training category proportions %%%%%%%
% Paddle size \textbackslash Background Color
\begin{table*} % Star does renders the table across two columns.
\centering
\begin{tabular}{r | c | c | c | c | c}
 Dataset & Frames & Red-Large & Red-Small & Blue-Large & Blue-Small \\
 \hline
 %D1 & 2458 & 0.422 / 0.553 & 0.090 / 0.000 & 0.095 / 0.282 & 0.393 / 0.074 \\
 %D1 intervention & 2557 & 0.426 / 0.605 & 0.385 / 0.000 & 0.084 / 0.594 & 0.105 / 0.000 \\
 D1 & 2313 & 0.5275 / \textbf{0.5328} & 0.000 / 0.000 & 0.000 / 0.000 & 0.4725 / \textbf{0.1775} \\
 D1 Intervention & 2205 & 0.000 / 0.000 & 0.3379 / \textbf{0.1624} & 0.6621 / \textbf{0.4521} & 0.000 / 0.000 \\
 Truncated D1 & 1156 & 0.5275 / \textbf{0.5328} & 0.000 / 0.000 & 0.000 / 0.000 & 0.4725 / \textbf{0.1775} \\ 
 Truncated D1 Intervention & 1102 & 0.000 / 0.000 & 0.3379 / \textbf{0.1624} & 0.6621 / \textbf{0.4521} & 0.000 / 0.000 \\
\end{tabular}
\caption{Dataset Info. Proportion of total data points (Pong frames) in category / Proportion of points in category belonging to winning games.}
\label{tab:dataset_proportions}
\end{table*}
%%%%%%%%%%%%

\begin{table*} % Star does renders the table across two columns.
\centering
\begin{tabular}{r | c | c | c | c | c}
 Model type & Dataset & Red-Large & Red-Small & Blue-Large & Blue-Small \\
 \hline
% D1 & 
% D1 intervention & 
No adversary & D1 & 0.5263 & - & - & 0.1871 \\
No adversary  & D1 Intervention & - & 0.2401 & 0.3243 & - \\
No adversary  & Truncated D1 & 0.5293 & - & - & 0.1890 \\
No adversary  & Truncated D1 Intervention & - & 0.3221 & 0.3007 & - \\
\hline
Adversary & D1 & 0.3892 & - & - & 0.3510 \\
Adversary  & D1 Intervention & - & 0.4590 & 0.2926 & - \\
Adversary  & Truncated D1 & 0.3946 & - & - & 0.3635 \\
Adversary  & Truncated D1 Intervention & - & 0.3159 & 0.3807 & -
\end{tabular}
\caption{Predicted winning probabilities learned by each model on each dataset, in each category. Dashes occur where there are zero data points in the given category. Compare with the bolded numbers in Table \ref{tab:dataset_proportions}.}
\label{tab:model_performance}
\end{table*}
%%%%%%%%%%%%



\section{Results}










\section{Conclusion}









\section{Acknowledgement}












%\nocite{*}
\bibliographystyle{named}
\bibliography{causality_l2m_bib}

\end{document}





%%%%%% Training category proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.095 & 0.422 \\
%\hline
%Small & 0.393 & 0.090
%\end{tabular}
%\caption{Training data category proportions among 2458 frames from 50 different games. Left column: large vs. small paddle size compared to the opponent agent. Top row: background color of the game. Generated from 20 games each with large paddle / red background and small paddle / blue background; 5 games each in the other two quadrants.}
%\label{tab:exp1_prop}
%\end{table}
%%%%%%%%%%%%%
%
%
%%%%%% Training win proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.282 & 0.553 \\
%\hline
%Small & 0.074 & 0.000
%\end{tabular}
%\caption{Training data win proportions among each combination of attributes. Each frame is annotated with the win condition of the game it is sampled from. Over half the frames containing a large paddle with red background were in a winning game. Although the background color does not affect gameplay, a very different proportion of frames with the large paddle ad blue background were in a winning game, mainly due to small sample size. Small sample size also explains why the lower right corner is zero.}
%\label{tab:exp1_win}
%\end{table}
%%%%%%%%%%%%%
%
%%%%%% Test category proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.079 & 0.376 \\
%\hline
%Small & 0.465 & 0.081
%\end{tabular}
%\caption{Test data proportions among 2726 frames. Compare to Table \ref{tab:exp1_prop}}
%\label{tab:exp1_test_prop}
%\end{table}
%%%%%%%%%%%%%
%
%
%%%%%% Test win proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.594 & 0.381 \\
%\hline
%Small & 0.000 & 0.000
%\end{tabular}
%\caption{Test data win proportions. Compare to Table \ref{tab:exp1_win}}
%\label{tab:exp1_test_win}
%\end{table}
%%%%%%%
%
%%%%%% Learned train proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.263 & 0.556 \\
%\hline
%Small & 0.072 & 0.000
%\end{tabular}
%\caption{Average win probabilities predicted by a basic neural net trained on the training data.}
%\label{tab:exp1_model}
%\end{table}
%%%%%%%%%%%%%%%%%%%
%
%%%%%% Learned test proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.277 & 0.627 \\
%\hline
%Small & 0.030 & 0.000
%\end{tabular}
%\caption{Average win probabilities predicted by a basic neural network on the test data. Note the proportions are more similar to the training data than to the actual test data proportions, as expected, since there is very little information in each frame to indicate a win besides color and paddle size; the network mainly defaults to its priors in those quadrants.}
%\label{tab:exp1_test_win}
%\end{table}
%%%%%%%%%%%%%
%
%%%%%% Intervention category proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.084 & 0.426 \\
%\hline
%Small & 0.105 & 0.385
%\end{tabular}
%\caption{Intervention data category proportions among 2557 frames in an interventional dataset.}
%\label{tab:exp1_int_prop}
%\end{table}
%%%%%%%%%%%%%
%
%
%%%%%% Intrevention win proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.594 & 0.605 \\
%\hline
%Small & 0.000 & 0.000
%\end{tabular}
%\caption{Intervention data win proportions.}
%\label{tab:exp1_int_win}
%\end{table}
%%%%%%%%%%%%%
%
%%%%%% Intrevention model test proportions %%%%%%%
%% Paddle size \textbackslash Background Color
%\begin{table}
%\centering
%\begin{tabular}{r | c | c}
% & Blue & Red \\
%\hline
%Large & 0.276 & 0.613 \\
%\hline
%Small & 0.051 & 0.000
%\end{tabular}
%\caption{Win probabilities as predicted by the neural network model.}
%\label{tab:exp1_int_model}
%\end{table}
%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Text list
% 
% Experimental and model design
% Results with and without adversarial training
% (Fix problem with small features)
% 
% 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure list
%
% Game images!
% 4-quadrant tables giving sample proportions
% Tables giving performance of different models in those quadrants
% Maybe training curves, maybe.
% 
% 
% 
% 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
